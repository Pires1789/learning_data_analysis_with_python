# Learning Python from Zero to Hero in Data Analysis

Hi!  
I created this repository with two objectives: **(a) to share my studies from zero to hero with Python focused on Data Analysis and (b) to practice my English**. Another important point is that I have experience with Python and other skills for Data Analysis (for example: SQL and PySpark) but I feel that I need to organize all the knowledge that I've accumulated over time. Well... I'm not a Hero or Zen Master in this subject, but an intermediate developer.

I plan to follow this schedule. 
It's important to let you know that I use AI (ChatGPT and Gemini) to create this step-by-step program.

Ah! I know that all the topics above are in Portuguese. Over time I will translate them into English - the AI could do all this work for me, but remember: one of my goals is to improve my English. :)


## Table of Contents
1. [Schedule made with ChatGPT](#schedule-made-with-chatgpt)
2. [Schedule made with Gemini](#schedule-made-with-gemini)

## Schedule made with ChatGPT

### Semana 1: Fundamentos de Python
- **Dia 1**: Introdução ao Python (Instalação, IDEs, Jupyter Notebooks)
- **Dia 2**: Sintaxe básica (variáveis, tipos de dados, operadores)
- **Dia 3**: Estruturas de controle (if, else, elif)
- **Dia 4**: Loops (for, while)
- **Dia 5**: Listas e tuplas
- **Dia 6**: Dicionários e conjuntos
- **Dia 7**: Revisão e exercícios práticos

### Semana 2: Funções e Manipulação de Dados
- **Dia 8**: Definição de funções e parâmetros
- **Dia 9**: Retorno de valores e funções aninhadas
- **Dia 10**: Manipulação de strings
- **Dia 11**: Compreensão de listas
- **Dia 12**: Manipulação de arquivos (leitura e escrita)
- **Dia 13**: Introdução ao módulo `os` e manipulação de diretórios
- **Dia 14**: Revisão e exercícios práticos

### Semana 3: Manipulação de Dados com Pandas
- **Dia 15**: Introdução ao Pandas (Instalação, importação, estrutura básica)
- **Dia 16**: Séries e DataFrames
- **Dia 17**: Leitura de dados (CSV, Excel, JSON)
- **Dia 18**: Seleção e filtragem de dados
- **Dia 19**: Manipulação de colunas e linhas
- **Dia 20**: Agrupamento e agregação de dados
- **Dia 21**: Revisão e exercícios práticos

### Semana 4: Limpeza e Preparação de Dados
- **Dia 22**: Tratamento de valores nulos
- **Dia 23**: Remoção e substituição de dados
- **Dia 24**: Identificação e tratamento de outliers
- **Dia 25**: Manipulação de datas e horários
- **Dia 26**: Mesclagem e junção de DataFrames
- **Dia 27**: Reindexação e ordenação
- **Dia 28**: Revisão e exercícios práticos

### Semana 5: Visualização de Dados com Matplotlib e Seaborn
- **Dia 29**: Introdução ao Matplotlib (plotagem básica)
- **Dia 30**: Gráficos de linhas, barras e dispersão
- **Dia 31**: Customização de gráficos (títulos, legendas, cores)
- **Dia 32**: Introdução ao Seaborn (instalação e conceitos)
- **Dia 33**: Gráficos de distribuição e categóricos
- **Dia 34**: Pairplots e heatmaps
- **Dia 35**: Revisão e exercícios práticos

### Semana 6: Estatística e Análise de Dados
- **Dia 36**: Fundamentos de estatística (média, mediana, moda, variância)
- **Dia 37**: Distribuições (normal, binomial)
- **Dia 38**: Testes de hipóteses
- **Dia 39**: Correlações e regressão linear
- **Dia 40**: Análise exploratória de dados (EDA)
- **Dia 41**: Análise de dados com Pandas e visualização
- **Dia 42**: Revisão e exercícios práticos

### Semana 7: Automação de Tarefas e Web Scraping
- **Dia 43**: Introdução à automação com Python
- **Dia 44**: Uso de loops e funções para automação
- **Dia 45**: Introdução ao Web Scraping com `requests` e `BeautifulSoup`
- **Dia 46**: Navegação em páginas web e extração de dados
- **Dia 47**: Automação de tarefas usando scripts Python
- **Dia 48**: Introdução ao `Selenium` para web scraping mais avançado
- **Dia 49**: Revisão e exercícios práticos

### Semana 8: Projeto Final e Preparação para o Mercado
- **Dia 50**: Definição de um projeto final (análise de um conjunto de dados real)
- **Dia 51**: Coleta e limpeza de dados para o projeto
- **Dia 52**: Análise exploratória e visualizações
- **Dia 53**: Aplicação de técnicas estatísticas e modelagem simples
- **Dia 54**: Conclusão do projeto e geração de insights
- **Dia 55**: Preparação do portfólio e upload no GitHub
- **Dia 56**: Revisão geral, organização dos códigos e práticas de boas práticas

## Schedule made with Gemini

### Fase 1: Fundamentos de Python (2-4 semanas)
- **Sintaxe básica**: Variáveis, tipos de dados, operadores, estruturas de controle (if, else, for, while).
- **Funções**: Definição, parâmetros, retorno.
- **Módulos e pacotes**: Importando e utilizando módulos padrão.
- **Recursos**:
  - **Cursos online**: Coursera, edX, Udemy, Codecademy.
  - **Livros**: "Python Crash Course" de Eric Matthes, "Automate the Boring Stuff with Python" de Al Sweigart.
  - **Documentação oficial**: [docs.python.org](https://docs.python.org)

### Fase 2: Estruturas de Dados (1-2 semanas)
- **Listas**: Criação, indexação, slicing, métodos.
- **Tuplas**: Características e diferenças das listas.
- **Dicionários**: Chaves e valores, operações.
- **Conjuntos**: Operações e aplicações.

### Fase 3: NumPy (1-2 semanas)
- **Arrays**: Criação, operações, indexação.
- **Broadcasting**: Operações entre arrays de diferentes tamanhos.
- **Funções universais**: Aplicando funções a elementos de um array.
- **Álgebra linear**: Operações matriciais.

### Fase 4: Pandas (2-3 semanas)
- **Series**: Criação, indexação, seleção.
- **DataFrames**: Criação, manipulação, visualização.
- **Leitura e escrita de dados**: CSV, Excel, bases de dados.
- **Limpeza e preparação de dados**: Tratamento de dados faltantes, outliers, transformações.

### Fase 5: Visualização de Dados (1-2 semanas)
- **Maplotlib**: Criação de gráficos básicos e personalizados.
- **Seeaborn**: Visualizações estatísticas de alto nível.
- **Plotly**: Gráficos interativos
-  
### Fase 6: Introdução à Estatística e Probabilidade (1-2 semanas)
- **Conceitos básicos**: Média, mediana, moda, desvio padrão, correlação.
- **Distribuições de probabilidade**: Normal, binomial, Poisson.
- **Testes de hipóteses**: T-test, ANOVA.

### Fase 7: Machine Learning (Tempo variável)
- **Scikit-learn**: Algoritmos de aprendizado supervisionado (regressão, classificação) e não supervisionado (clustering).
- **Pré-processamento de dados**: Escalonamento, normalização, seleção de features.
- **Avaliação de modelos**: Métricas de desempenho, validação cruzada.
